for (i in 1:length(urls_codes)) {
#lang_names[[length(urls_codes)+1]] = obtener_titulo(urls_codes[i])
print(urls_codes[i])
}
for (i in 1:4) {
#lang_names[[length(urls_codes)+1]] = obtener_titulo(urls_codes[i])
html <- read_html(urls_codes[i])
titulo = html %>%
html_nodes(xpath="//*[@id='block-system-main']/div/div/div/div[1]/div/div/div[3]/div[2]/div")%>%
html_text()
print(titulo)
}
#length(urls_codes)
lang_names <- list()
for (i in 1:4) {
#lang_names[[length(urls_codes)+1]] = obtener_titulo(urls_codes[i])
html <- read_html(urls_codes[i])
titulo = html %>%
html_nodes(xpath="//*[@id='block-system-main']/div/div/div/div[1]/div/div/div[3]/div[2]/div")%>%
html_text()
lang_names[[length(urls_codes)+1]] = titulo
}
lang_names
#length(urls_codes)
lang_names <- list()
for (i in 1:length(urls_codes)) {
#lang_names[[length(urls_codes)+1]] = obtener_titulo(urls_codes[i])
html <- read_html(urls_codes[i])
titulo = html %>%
html_nodes(xpath="//*[@id='block-system-main']/div/div/div/div[1]/div/div/div[3]/div[2]/div")%>%
html_text()
lang_names[[length(urls_codes)]] = titulo
print(titulo)
}
lang_names
length(codes)
length(lang_names)
length(urls_codes)
iso_codes_db <- data_frame(
codigos = codes,
nombre = lang_names,
url = urls_codes
)
View(iso_codes_db)
View(iso_codes_db)
for (i in 1:length(urls_codes)) {
#lang_names[[length(urls_codes)+1]] = obtener_titulo(urls_codes[i])
html <- read_html(urls_codes[i])
titulo = html %>%
html_nodes(xpath="//*[@id='block-system-main']/div/div/div/div[1]/div/div/div[3]/div[2]/div")%>%
html_text()
lang_names[[length(urls_codes)]] = titulo
print(lang_names)
}
#length(urls_codes)
lang_names <- list()
for (i in 1:length(urls_codes)) {
lang_names[[length(urls_codes)+1]] = obtener_titulo(urls_codes[i])
#lang_names[[length(urls_codes)]] = titulo
print(lang_names)
}
lang_names
View(lang_names)
#Read website with all iso-codes of Mexican languages
pg <- read_html("https://www.ethnologue.com/country/MX/languages")
#Extract iso-codes
codes <- pg2 %>%
html_nodes(xpath="//div[@class='content']/a")%>%
html_text()
list(codes)
#Extract iso-codes
iso_codes <- pg %>%
html_nodes(xpath="//div[@class='content']/a")%>%
html_text()
#Create a list of iso_codes
single_codes <- list(iso_codes)
#Create a list of iso_codes
codes <- list(iso_codes)
#Specifying the url for desired website to be scrapped
pg <- read_html("https://www.gob.mx/presidencia/archivo/prensa?utf8=%E2%9C%93&idiom=es&style=list&order=DESC&filter_id=&filter_origin=archive&tags=&year=&category=Discursos+del+Presidente&year=&category=Discursos+del+Presidente")
pg
pg2 <- gsub("\\\\", "", pg)
pg2
pg2 <- gsub("\\\\\\\\", "", pg)
pg2
pg2 <- gsub("\\\\", "", pg)
pg2
?read_html
#Specifying the url for desired website to be scrapped
pg <- read_html("https://www.gob.mx/presidencia/archivo/prensa?utf8=%E2%9C%93&idiom=es&style=list&order=DESC&filter_id=&filter_origin=archive&tags=&year=&category=Discursos+del+Presidente&year=&category=Discursos+del+Presidente", encoding="ascii")
pg
#Specifying the url for desired website to be scrapped
pg <- read_html("https://www.gob.mx/presidencia/archivo/prensa?utf8=%E2%9C%93&idiom=es&style=list&order=DESC&filter_id=&filter_origin=archive&tags=&year=&category=Discursos+del+Presidente&year=&category=Discursos+del+Presidente", encoding="utf-8")
pg
pg2
pg2 <- gsub("\\\\\\\\", "", pg)
pg2
dias <- list(1:15)
dias
url_base <- "http://www.jornada.unam.mx/ultimas/?came_from=http%3A//www.jornada.unam.mx/ultimas/2017/11/"
dias <- list(1:15)
quincenanov <- paste0(rep(url_base,length(dias)),dias)
quincenanov
quincenanov <- paste0(rep(url_base,length(15)),1:15)
quincenanov
prueba %>%
html_nodes(".carrusel_secundario_nuevo_item_title a div")
prueba <- read_html("http://www.jornada.unam.mx/ultimas/?came_from=http%3A//www.jornada.unam.mx/ultimas/2017/11/1")
prueba %>%
html_nodes(".carrusel_secundario_nuevo_item_title a div")
prueba %>%
html_node(".carrusel_secundario_nuevo_item_title a div")
prueba %>%
html_node(".carrusel_secundario_nuevo_item_title a")
prueba %>%
html_node(".carrusel_secundario_nuevo_item_title a") %>%
html_attr(href)
prueba %>%
html_node(".carrusel_secundario_nuevo_item_title a") %>%
html_attr("href")
prueba <- read_html("http://www.jornada.unam.mx/ultimas/?came_from=http%3A//www.jornada.unam.mx/ultimas/2017/11/15")
prueba %>%
html_node(".carrusel_secundario_nuevo_item_title a") %>%
html_attr("href")
prueba <- read_html("http://www.jornada.unam.mx/ultimas/?came_from=http%3A//www.jornada.unam.mx/ultimas/2017/11/15")
prueba %>%
html_node(".carrusel_secundario_nuevo_item_title a") %>%
html_attr("href")
length(lang_names)
View(lang_names)
obtener_titulo(zai)
#test one language
zai <- read_html("https://www.ethnologue.com/language/zai")
#test one language
zai <- "https://www.ethnologue.com/language/zai"
#Create function to extract each language's name
obtener_titulo = function(url)
{
# leer html
html <- read_html(url)
# leer título
titulo = html %>%
html_nodes(xpath="//*[@id='block-system-main']/div/div/div/div[1]/div/div/div[3]/div[2]/div")%>%
html_text()
return(titulo)
}
obtener_titulo(zai)
#Populate list with each
lang_names <- list()
for (i in 1:length(urls_codes)) {
print(urls_codes[i])
lang_names[[length(urls_codes)+1]] = obtener_titulo(urls_codes[i])
print(obtener_titulo(urls_codes[i]))
}
lang_names
a <- readLines("https://www.gob.mx/presidencia/archivo/prensa?utf8=%E2%9C%93&idiom=es&style=list&order=DESC&filter_id=&filter_origin=archive&tags=&year=&category=Discursos+del+Presidente&year=&category=Discursos+del+Presidente", encoding="utf-8")
a
pg2 <- gsub("\\\\\\\\", "", pg)
#Specifying the url for desired website to be scrapped
pg <- read_html("https://www.gob.mx/presidencia/archivo/prensa?utf8=%E2%9C%93&idiom=es&style=list&order=DESC&filter_id=&filter_origin=archive&tags=&year=&category=Discursos+del+Presidente&year=&category=Discursos+del+Presidente", encoding="utf-8")
pg2 <- gsub("\\\\\\\\", "", pg)
pg3 <- gsub("\\\\", "", pg)
pg2 <- gsub("\\\\", "", pg)
pg3 <- gsub("\\\\\\\\", "", pg)
pg2
head(pg)
pg3
lang_names
cat("\014")
#Loading the rvest package
#install.packages("rvest")
library('rvest')
#Specifying the url for desired website to be scrapped
pg <- read_html("https://www.gob.mx/presidencia/archivo/prensa?utf8=%E2%9C%93&idiom=es&style=list&order=DESC&filter_id=&filter_origin=archive&tags=&year=&category=Discursos+del+Presidente&year=&category=Discursos+del+Presidente", encoding="utf-8")
pg2 <- gsub("\\\\", "", pg)
pg2
pg
pg3 <- gsub("\\\\\\\\", "", pg)
pg3
?read_html
typeof(pg)
pg4 <- gsub("\\\\", "", pg2)
pg4
str1 <- "<div class='\"col-md-12' small-bottom-buffer>"
str2 <- gsub("\\\\", "", str1)
str2
str1 <- "<div class='\"col-md-12' small-bottom-buffer>"
str2 <- gsub("\\", "", str1)
str1 <- "<div class='\"col-md-12' small-bottom-buffer>"
str2 <- gsub("\\\\", "", str1)
str2
getwd()
df <- read_csv("proyectos_opa.csv")
?read_csv
library(readr)
install.packages("readr")
install.packages("readr")
library(readr)
library("readr")
df <- read_csv("proyectos_opa.csv")
install.packages("tidyverse")
install.packages("tidyverse")
library("tidyverse")
df <- read_csv("proyectos_opa.csv")
?tidyverse
??tidyverse
install.packages("tidyverse")
install.packages("tidyverse")
library("tidyverse")
df <- read_csv("proyectos_opa.csv")
df
View(df)
inversion <- df$MONTO_TOTAL_INVERSION
quantile(inversion, prob = seq(0, 1, length = 11), type = 5)
deciles <- quantile(inversion, prob = seq(0, 1, length = 11), type = 5)
for (i in 1:length(deciles)) {
print(deciles[i])
}
for (i in 1:length(deciles)) {
print("hola", deciles[i])
}
for (i in 1:length(deciles)) {
print("hola" deciles[i])
}
print(deciles[i])
for (i in 1:length(deciles)) {
print(deciles[i])
}
deciles
deciles <- quantile(inversion, prob = seq(0, 1, length = 11), type = 5)
deciles
for (i in 1:length(deciles)) {
print(deciles[i])
}
inversion2 <- transform(inversion, decile = cut(prob, c(-Inf, deciles), labels = 0:10))
inversion2 <- transform(inversion, decile = cut(MONTO_TOTAL_INVERSION, c(-Inf, deciles), labels = 0:10))
inversion <- df$MONTO_TOTAL_INVERSION
deciles <- quantile(inversion, prob = seq(0, 1, length = 11), type = 5)
inversion2 <- transform(inversion, decil = cut(MONTO_TOTAL_INVERSION, c(-Inf, deciles), labels = 0:10))
inversion2 <- transform(inversion, decil = cut(MONTO_TOTAL_INVERSION, c(-Inf, deciles), labels = 0:10))
getwd()
ccmacrosims <- read.csv("ocr_tipos.csv")
ocr
ocr <- read.csv("ocr_tipos.csv")
ocr
getwd()
ocr <- read.csv("ocr_tipos.csv")
ocr
library("ggplot")
library("ggplot2")
df <- as.data.frame(ocr)
df
q1 <- ggplot(subset(df,group == 1),aes(x = reorder(fac,val),y = val)) +
geom_bar() +
facet_wrap(~group) +
coord_flip()
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon)) +
geom_jitter() +
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon)) +
geom_bar() +
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon)) +
geom_jitter() +
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_jitter() +
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_line() +
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon)) +
geom_line() +
facet_wrap(~ software)
geom_jitter
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_jitter() +
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(colour="tipo",stat = "identity")+
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
scale_x_discrete(limits = df$tipo[order(df$porcentaje_palabras_recon)])
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
scale_x_discrete(limits = df$tipo[order(df$porcentaje_palabras_recon)])+
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
scale_x_discrete(limits = df$porcentaje_palabras_recon[order(df$porcentaje_palabras_recon)])+
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
scale_x_discrete(limits = df[order(df$porcentaje_palabras_recon)])+
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
scale_x_discrete(limits = df$software[order(df$porcentaje_palabras_recon)])+
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
scale_x_discrete(limits = df$tipo[order(df$porcentaje_palabras_recon)])+
facet_wrap(~ software)
ggplot(df, aes(x = reorder(tipo, -porcentaje_palabras_recon), y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
#scale_x_discrete(limits = df$tipo[order(df$porcentaje_palabras_recon)])+
facet_wrap(~ software)
ggplot(df, aes(x = reorder(tipo, porcentaje_palabras_recon), y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
#scale_x_discrete(limits = df$tipo[order(df$porcentaje_palabras_recon)])+
facet_wrap(~ software)
source('~/.active-rstudio-document', echo=TRUE)
View(df)
getwd()
library("ggplot2")
ocr <- read.csv("ocr_tipos.csv")
df <- as.data.frame(ocr)
View(df)
ggplot(df, aes(x = reorder(tipo, porcentaje_palabras_recon), y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
facet_wrap(~ software)
ggplot(df, aes(x = reorder(tipo), y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
facet_wrap(~ software)
ggplot(df, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
facet_wrap(~ software)
df$tipo <- factor(df$tipo, levels = df$tipo[order(tipo$val)])
df$tipo <- factor(df$tipo, levels = df$tipo[order(x$val)])
df$tipo2 <- factor(df$tipo, levels = df$tipo[order(df$val)])
ocr <- read.csv("ocr_tipos.csv")
df <- as.data.frame(ocr)
df$tipo2 <- factor(df$tipo, levels = df$tipo[order(df$val)])
#population[order(population$age),]
df2 <- df[order(df$tipo)]
#population[order(population$age),]
df2 <- df[order(df$tipo),]
View(df)
View(df2)
ggplot(df2, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo)) +
geom_bar(stat = "identity")+
facet_wrap(~ software)
ggplot(df2, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo, fill=tipo)) +
geom_bar(stat = "identity")+
facet_wrap(~ software)
df3 <- df[order(df$porcentaje_palabras_recon),]
ggplot(df3, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo, fill=tipo)) +
geom_bar(stat = "identity")+
facet_wrap(~ software)
View(df3)
ggplot(df3, aes(x = tipo, y = porcentaje_palabras_recon, color = tipo, fill=tipo)) +
geom_bar(stat = "identity")+
facet_wrap(~ software)
library('rvest')
pg <- read_html("http://www.comunicacion.cdmx.gob.mx/discursos")
pg
prueba %>%
html_node(".pub")
prueba %>%
html_attr("href")
pg %>%
html_attr("href")
pg %>%
html_node(".pub")
pg %>%
html_node(".pub")%>%
html_attr("href")
pg %>%
html_node(".title")
html_attr("href")
#install.packages("rvest")
library('rvest')
pg <- read_html("http://www.comunicacion.cdmx.gob.mx/discursos")
pg %>%
html_node(".title")%>%
html_attr("href")
pg %>%
html_node(".title")
pg %>%
html_node(".pub a")
pg %>%
html_node(".pub a")%>%
html_attr("href")
pg %>%
html_nodes(".pub a")%>%
html_attr("href")
url_base <- "http://www.jornada.unam.mx/ultimas/?came_from=http%3A//www.jornada.unam.mx/ultimas/2017/11/"
quincenanov <- paste0(rep(url_base,length(15)),1:15)
quincenanov
discursos <- paste0(rep(pg,length(233)),1:233)
discursos
discursos <- paste0(rep(url_base,length(15)),1:15)
discursos
url_base <- "http://www.comunicacion.cdmx.gob.mx/discursos/"
url <- "http://www.comunicacion.cdmx.gob.mx/discursos/"
discursos <- paste0(rep(url,length(15)),1:15)
discursos
discursos <- paste0(rep(url,length(233)),1:233)
discursos
obtener_discursos = function(liga)
{
# leer html
pagina <- read_html(liga)
# leer título
pagina %>%
html_nodes(".pub a")%>%
html_attr("href")
return(titulo)
}
discursos[1]
obtener_discursos(discursos[1])
discursos[1]
obtener_discursos(discursos[1])
obtener_discursos("http://www.comunicacion.cdmx.gob.mx/discursos/1")
obtener_discursos("http://www.comunicacion.cdmx.gob.mx/discursos/2")
obtener_discursos = function(liga)
{
# leer html
pagina <- read_html(liga)
# leer título
ligas <- pagina %>%
html_nodes(".pub a")%>%
html_attr("href")
return(ligas)
}
obtener_discursos(discursos[1])
todas_ligas <- list()
for (i in 1:3) {
todas_ligas[[i+1]] = obtener_discursos(discursos[i])
#lang_names[[length(urls_codes)]] = titulo
#print(lang_names)
}
todas_ligas
discursos[:3]
discursos[,3]
discursos[3]
discursos[1:3]
todas_ligas <- list()
for (i in 1:1:length(prueba)) {
todas_ligas[[length(prueba)+1]] = obtener_discursos(discursos[i])
}
todas_ligas
pg %>%
html_nodes(".pub a")%>%
html_attr("href")
grepl("http://www.comunicacion.cdmx.gob.mx/noticias/etiqueta/capital-en-desarrollo", noticias)
grepl("http://www.comunicacion.cdmx.gob.mx/noticias/etiqueta/capital-en-desarrollo", "noticias")
?grepl
grepl("noticias", "http://www.comunicacion.cdmx.gob.mx/noticias/etiqueta/capital-en-desarrollo")
urls <- obtener_discursos(discursos[3])
for (i in 1:length(urls)) {
print(i)
if(grepl("nota",i){
print(i)
}
}
grepl("nota", "http://www.comunicacion.cdmx.gob.mx/noticias/etiqueta/capital-en-desarrollo")
prueba <- discursos[1:3]
todas_ligas <- list()
for (i in 1:1:length(prueba)) {
todas_ligas[[length(prueba)+1]] = obtener_discursos(discursos[i])
}
todas_ligas
urls <- obtener_discursos(discursos[3])
for (i in 1:length(urls)) {
print(i)
if(grepl("nota",i){
print(i)
}
}
grepl("nota", "http://www.comunicacion.cdmx.gob.mx/noticias/etiqueta/capital-en-desarrollo")
prueba <- discursos[1:3]
todas_ligas <- list()
for (i in 1:1:length(prueba)) {
todas_ligas[[length(prueba)+1]] = obtener_discursos(discursos[i])
}
todas_ligas
for (i in 1:length(urls)) {
if(grepl("nota",i){
print(i)
}
}
for (i in 1:length(urls)){
if(grepl("nota",i){
print(i)
}
}
library(rJava)
install.packages("rJava")
library(NLP)
install.packages("NLP")
install.packages("openNLP")
library(rJava)
install.packages("rJava")
library(rJava)
library("rJava")
mtcars[c(3, 24),]
mtcars[c(6, 24),]
mtcars[c(1, 24),]
mtcars[.3]
mtcars[,3]
mtcars[1:5,]
freqbook[1:20,]
freqbooks[1:20,]
# set working directory
#C:\Users\segutierrez\Dropbox\eswiki
setwd("/Users/segutierrez/Dropbox/eswiki")
eswiki <- read.table(file = 'eswiki.tsv', sep = '\t', header = TRUE, fill=TRUE, encoding = "UTF-8")
